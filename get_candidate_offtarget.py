#!/usr/bin/env python3

"""
get_candidate_offtarget.py

Function:
1. Read the 4 result files generated by TDOscanner.
2. Extract the 'upstream20bp' sequence from each file.
3. Search for potential off-target sites (Mismatch=0 and Mismatch=1) in the whole genome using multi-processing.
   Definition: 20bp upstream of NGG or 20bp downstream of CCN.
4. Output new files containing off-target statistics appended to the original data.
"""

import argparse
import sys
import os
import glob
import multiprocessing
import pysam
from collections import defaultdict

# -----------------------------------------------------------------------------
# Utils
# -----------------------------------------------------------------------------

def reverse_complement(seq):
    complement = str.maketrans('ATCGNatcgn', 'TAGCNtagcn')
    return seq.translate(complement)[::-1]

def generate_1mm_variants(seq):
    """Generate all 1-mismatch variants for a given sequence."""
    variants = set()
    bases = ['A', 'C', 'G', 'T']
    seq_list = list(seq)
    for i in range(len(seq)):
        original_char = seq_list[i]
        for b in bases:
            if b != original_char:
                seq_list[i] = b
                variants.add("".join(seq_list))
        seq_list[i] = original_char # restore
    return variants

def parse_input_files(file_list):
    """
    Read input files and collect query sequences.
    Returns: A set of unique sequences to query.
    """
    query_seqs = set()
    print("Reading input files to collect query sequences...")
    for fname in file_list:
        if not os.path.exists(fname):
            print(f"Warning: File {fname} not found.")
            continue
        
        with open(fname, 'r') as f:
            header = f.readline()
            # Determine the column index for 'upstream20bp'
            headers = header.strip().split('\t')
            try:
                # Try to find by name, default to last column if not found
                col_idx = -1
                for i, h in enumerate(headers):
                    if "upstream20bp" in h:
                        col_idx = i
                        break
            except:
                col_idx = -1

            for line in f:
                parts = line.strip().split('\t')
                if len(parts) <= abs(col_idx): continue
                seq = parts[col_idx].upper()
                
                # Filter invalid sequences
                if len(seq) == 20 and 'N' not in seq:
                    query_seqs.add(seq)
    
    print(f"Total unique query sequences to search: {len(query_seqs)}")
    return query_seqs

def scan_chromosome(args):
    """
    Worker function: Scan a single chromosome.
    args: (chrom_name, fasta_path, interesting_seqs_set)
    """
    chrom, fasta_path, target_set = args
    hits = defaultdict(list)  # {sequence: ["chr:start-end:strand", ...]}
    
    fasta = pysam.FastaFile(fasta_path)
    try:
        seq = fasta.fetch(chrom).upper()
    except:
        return hits
    
    seq_len = len(seq)
    
    # ---------------------------------------------------------
    # 1. Forward Strand Logic (Look for [20bp] + NGG)
    # ---------------------------------------------------------
    # The PAM (NGG) starts at index i+21. 
    # So sequence is at i to i+20.
    # N is at i+20. G at i+21, G at i+22.
    # Loop i from 0 to seq_len - 23
    
    # Optimization: Find 'GG' directly, then check the preceding sequence.
    # str.find('GG') is faster than regex for large strings.
    
    pos = 0
    while True:
        # Find 'GG' which corresponds to the 2nd and 3rd base of PAM
        pos = seq.find('GG', pos)
        if pos == -1: break
        
        # PAM is NGG. 'GG' is at pos, pos+1.
        # N is at pos-1.
        # 20bp target ends at pos-2. 
        # 20bp target starts at pos-1 - 20 = pos - 21.
        
        start_idx = pos - 21
        if start_idx >= 0:
            candidate = seq[start_idx : start_idx + 20]
            if candidate in target_set:
                # 1-based coords: start is start_idx + 1
                # end is start_idx + 20
                loc_str = f"{chrom}:{start_idx+1}-{start_idx+20}:+"
                hits[candidate].append(loc_str)
        
        pos += 1 # Move forward to find next occurrence

    # ---------------------------------------------------------
    # 2. Reverse Strand Logic (Look for CCN + [20bp])
    # On the reference strand, this looks like CC + N + [Target Sequence]
    # The target sequence on the genome is the Reverse Complement of the sgRNA spacer.
    # However, since we stored the 'upstream20bp' (which represents the spacer),
    # if the match is on the (-) strand, the genome sequence will be the RC of the spacer.
    #
    # Validation: The spacer (upstream20bp) matches the DNA 5'->3'.
    # If the target is on (-) strand:
    # Genome (5'->3'): ... Target_RC ... NGG_RC (CCN) ...
    # This means we find CCN, and the following 20bp is the Target_RC.
    # We then RC that found chunk to recover the Target Spacer for comparison.
    # ---------------------------------------------------------
    
    pos = 0
    while True:
        # Find 'CC' (start of CCN)
        pos = seq.find('CC', pos)
        if pos == -1: break
        
        # CCN starts at pos.
        # CC at pos, pos+1. N at pos+2.
        # 20bp target starts at pos+3.
        # 20bp target ends at pos+3+20 = pos+23.
        
        start_idx = pos + 3
        end_idx = pos + 23
        
        if end_idx <= seq_len:
            raw_chunk = seq[start_idx : end_idx]
            # This raw matching chunk is on the + strand reference. 
            # But it represents a match on the - strand.
            # To compare with our query (which is the spacer), we need to RC this chunk.
            candidate = reverse_complement(raw_chunk)
            
            if candidate in target_set:
                # Coords: still use the genomic coordinates on reference
                # Note: For reverse strand features, standard format is start < end.
                loc_str = f"{chrom}:{start_idx+1}-{end_idx}:-"
                hits[candidate].append(loc_str)
        
        pos += 1

    return hits

# -----------------------------------------------------------------------------
# Main Class
# -----------------------------------------------------------------------------

class OffTargetScanner:
    def __init__(self, fasta_path, input_files):
        self.fasta = fasta_path
        self.input_files = input_files
        self.target_files = {
            'gene_t1': "output_gene_type1.txt",
            'gene_t2': "output_gene_type2.txt",
            'rna_t1': "output_matureRNA_type1.txt",
            'rna_t2': "output_matureRNA_type2.txt"
        }
        # Override input files if provided explicitly, else assume defaults
        if not input_files:
            self.inputs = [f for f in self.target_files.values() if os.path.exists(f)]
        else:
            self.inputs = input_files

    def run(self):
        # 1. Collect Queries
        raw_queries = parse_input_files(self.inputs)
        if not raw_queries:
            print("No valid queries found. Exiting.")
            sys.exit(0)
        
        # 2. Build Search Set (Query + 1MM variants)
        print("Generating 1-mismatch variants...")
        search_set = set()
        
        # Map: Variant -> Original(s) it belongs to. 
        # One variant could be a 1MM of Query A and exactly Query B.
        # Here we just need to know "Is this sequence in the genome interesting?"
        # Later we explicitly check MM0 and MM1 counts for each Query.
        for q in raw_queries:
            search_set.add(q) # MM0
            variants = generate_1mm_variants(q) # MM1
            search_set.update(variants)
        
        print(f"Total sequences in search index (Queries + Variants): {len(search_set)}")
        
        # 3. Multiprocessing Genome Scan
        print("Scanning genome (this may take a while)...")
        # Get Chromosome names
        with pysam.FastaFile(self.fasta) as f:
            chroms = f.references
        
        pool_args = [(chrom, self.fasta, search_set) for chrom in chroms]
        
        # Use roughly 75% of available cores
        n_cpu = max(1, int(multiprocessing.cpu_count() * 0.75))
        print(f"Using {n_cpu} threads.")
        
        global_hits = defaultdict(list)
        
        with multiprocessing.Pool(n_cpu) as pool:
            # imap_unordered is good for progress monitoring
            for chrom_hits in pool.imap_unordered(scan_chromosome, pool_args):
                for seq, locs in chrom_hits.items():
                    global_hits[seq].extend(locs)
        
        print("Genome scan complete. Aggregating results...")
        
        # 4. Process each file and append results
        self.write_outputs(global_hits)
        print("All done.")

    def format_locations(self, loc_list):
        """Format location list based on count threshold (<= 50)."""
        count = len(loc_list)
        if count == 0:
            return "0", "-"
        elif count > 50:
            return str(count), "-"
        else:
            return str(count), ";".join(loc_list)

    def write_outputs(self, global_hits):
        header_suffix = "\tMM0_Count\tMM1_Count\tMM0_Locations\tMM1_Locations\n"
        
        for fname in self.inputs:
            print(f"Processing results for {fname}...")
            out_name = os.path.splitext(fname)[0] + "_w_offtarget.txt"
            
            with open(fname, 'r') as fin, open(out_name, 'w') as fout:
                # Header
                header = fin.readline().strip()
                fout.write(header + header_suffix)
                
                # Detect column index for 'upstream20bp'
                cols = header.split('\t')
                try:
                    c_idx = -1
                    for i, c in enumerate(cols):
                        if "upstream20bp" in c:
                            c_idx = i
                            break
                    if c_idx == -1: c_idx = -1 # Default to last column
                except:
                    c_idx = -1

                for line in fin:
                    line = line.strip()
                    parts = line.split('\t')
                    if not parts: continue
                    
                    # Extract query sequence
                    try:
                        query = parts[c_idx].upper()
                    except IndexError: # Empty line or bad format
                        fout.write(line + "\t0\t0\t-\t-\n")
                        continue
                        
                    if 'N' in query or len(query) != 20:
                        fout.write(line + "\t0\t0\t-\t-\n")
                        continue

                    # Retrieve MM0 hits directly from global_hits
                    mm0_hits = global_hits.get(query, [])
                    mm0_count_str, mm0_loc_str = self.format_locations(mm0_hits)
                    
                    # Retrieve MM1 hits
                    # We need to regenerate matched variants for this specific query
                    # and check if they exist in global_hits.
                    current_variants = generate_1mm_variants(query)
                    mm1_hits_all = []
                    for v in current_variants:
                        if v in global_hits:
                            mm1_hits_all.extend(global_hits[v])
                    
                    mm1_count_str, mm1_loc_str = self.format_locations(mm1_hits_all)
                    
                    fout.write(f"{line}\t{mm0_count_str}\t{mm1_count_str}\t{mm0_loc_str}\t{mm1_loc_str}\n")
            
            print(f"Written: {out_name}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Find candidate CRISPR off-targets for TDOscanner results")
    parser.add_argument("fasta", help="Reference Genome FASTA file")
    parser.add_argument("files", nargs='*', help="Input result files (optional, defaults to standard names)")
    
    args = parser.parse_args()
    
    scanner = OffTargetScanner(args.fasta, args.files)
    scanner.run()
